<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>aws on Alfred Nutile</title><link>https://alfrednutile.info/tags/aws/</link><description>Recent content in aws on Alfred Nutile</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 06 Feb 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://alfrednutile.info/tags/aws/index.xml" rel="self" type="application/rss+xml"/><item><title>Using Python Lambda behind and ALB</title><link>https://alfrednutile.info/posts/266/</link><pubDate>Thu, 06 Feb 2020 00:00:00 +0000</pubDate><guid>https://alfrednutile.info/posts/266/</guid><description><p>Just a quick note to self about ALB and Lambda. When using Lambda behind and ALB for routing your response has to be formatted properly else you will get a 502.</p><p><a href="https://pypi.org/project/alb-response">https://pypi.org/project/alb-response/</a> solved this problem.</p><pre><code>from alb_response import alb_response<p>def lambda_handler(event, context):</p><pre><code>response_dict = process_the_event(event)
return alb_response(
http_status=200,
json=response_dict,
is_base64_encoded=False,
)</code></pre><p/></pre><p>The results are easy enough to do by hand but I had a bit of time really finding out the format to respond with other than JavaScript and that casing there was a bit confusing.</p></p></description><tags>aws, serverless, note2self</tags></item><item><title>Lambda Tips</title><link>https://alfrednutile.info/posts/264/</link><pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate><guid>https://alfrednutile.info/posts/264/</guid><description><p>@WIP</p><h2 id="taking-advantage-of-a-running-lambda-function-and-its-state">Taking advantage of a running Lambda function and it&rsquo;s state</h2><p><a href="https://aws.amazon.com/blogs/compute/sharing-secrets-with-aws-lambda-using-aws-systems-manager-parameter-store/">https://aws.amazon.com/blogs/compute/sharing-secrets-with-aws-lambda-using-aws-systems-manager-parameter-store/</a> under the section<strong>Lambda function</strong> has a nice &ldquo;trick&rdquo; of setting above the class<code>app = None</code></p><p>then later on it will see if that is set</p><pre><code>def lambda_handler(event, context):
global app
# Initialize app if it doesn't yet exist
if app is None:
print("Loading config and creating new MyApp...")
config = load_config(full_config_path)
app = MyApp(config)<pre><code>return &amp;quot;MyApp config is &amp;quot; + str(app.get_config()._sections)</code></pre><p/></pre><p>If it is set it will not try to set it again but take advantage of the state and use it.</p></p><h2 id="keep-warm">Keep Warm</h2><p><a href="https://read.acloud.guru/how-to-keep-your-lambda-functions-warm-9d7e1aa6e2f0">https://read.acloud.guru/how-to-keep-your-lambda-functions-warm-9d7e1aa6e2f0</a></p><p>You can set a bunch of schedulers and your Lambda function can check for the context of the request. If it is a scheduler event then just reply OK otherwise it should do what it normally would do.</p><pre><code>import boto3
from config import Config<p>class KeepAwake:
def<strong>init</strong>(self):
""" keey awake """
self.config = Config()
self.region = self.config.region
self.app_env = self.config.app_env
self.client = boto3.client(&lsquo;lambda&rsquo;, region_name=self.region)
self.functions = [
"foo",
"bar",
]</p><pre><code>def run(self):
&amp;quot;&amp;quot;&amp;quot; interate over lambda functions &amp;quot;&amp;quot;&amp;quot;
for lam in self.functions:
print(&amp;quot;Invoking &amp;quot;, lam)
self.client.invoke(
FunctionName=lam,
InvocationType=&amp;quot;Event&amp;quot;
)
print(&amp;quot;Invoked &amp;quot;, lam)</code></pre><p/></pre><p>Is another way to look around and call those functions.</p></p></description><tags>note2self, aws, python</tags></item><item><title>Cognito and OAuth</title><link>https://alfrednutile.info/posts/261/</link><pubDate>Thu, 14 Mar 2019 00:00:00 +0000</pubDate><guid>https://alfrednutile.info/posts/261/</guid><description><p>make your App Client</p><p>Make sure to check</p><p><img src="https://dl.dropboxusercontent.com/s/b41fgm4avehxkar/cog_app_client.png?dl=0" alt=""/><p>Make some scopes uner Resource Server</p><p>Then &ldquo;App Client Settings&rdquo;</p><p>Connect it to &ldquo;Cognito User Pool&rdquo;</p><p>And choose &ldquo;Client Credentials&rdquo; from &ldquo;Allowed OAuth Flow&rdquo; choosing some scopes</p></description><tags>aws, cognito, laravel</tags></item><item><title>Deploying Fargate</title><link>https://alfrednutile.info/posts/259/</link><pubDate>Sat, 27 Oct 2018 00:00:00 +0000</pubDate><guid>https://alfrednutile.info/posts/259/</guid><description><p>Just some notes on the process. This will show how to deploy both a Staging build and then Production.</p><h2 id="staging">Staging</h2><p>Staging is done by TravisCI after all tests pass</p><p>Here is the gist of it the deploy step calls to a bash file.</p><pre><code>deploy:
skip_cleanup: true
provider: script
script: bash deploy/travis_deploy.sh
on:
branch: mainline</code></pre><p>Then</p><pre><code>#!/usr/bin/env bash<h1 id="bail-out-on-first-error">Bail out on first error</h1><p>set -e</p><h2 id="get-the-directory-of-the-build-script">Get the directory of the build script</h2><p>DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &amp;&amp; pwd )"</p><h2 id="get-the-current-git-commit-sha">Get the current git commit sha</h2><p>HASH=$(git rev-parse HEAD)</p><h2 id="get-any-secret-files">Get any secret files</h2><p>aws s3 cp s3://foo/environments/$STACK_ENV_FILE $DIR/app/packaged/.env</p><p>##we only want non-dev vendors
composer config -g github-oauth.github.com $GITHUB_TOKEN &amp;&amp; composer install &ndash;no-dev</p><p>echo "Region $STACK_AWS_REGION is the target region"
eval $(aws ecr get-login &ndash;no-include-email &ndash;region $STACK_AWS_REGION)
echo "Tagging images $STACK_APP_NAME"
docker build &ndash;pull -t $STACK_APP_NAME .
docker tag $STACK_APP_NAME:latest 1111111111.dkr.ecr.$STACK_AWS_REGION.amazonaws.com/$STACK_APP_NAME:latest
echo "Pushing up image $STACK_APP_NAME:latest"
docker push 1111111111.dkr.ecr.us-east-1.amazonaws.com/$STACK_APP_NAME:latest</p><h2 id="now-run-again-for-production-will-come-back-to-this-in-a-moment">Now Run again for Production WILL COME BACK TO THIS IN A MOMENT</h2><h2 id="if-production-set">if production set???</h2><p>if [[ "$STACK_ENV_FILE_PRODUCTION" ]]; then
echo "Running Production build"
aws s3 cp s3://foo/environments/$STACK_ENV_FILE_PRODUCTION $DIR/app/packaged/.env
echo "Building Production Image"
docker build &ndash;pull -t $STACK_APP_NAME .
docker tag $STACK_APP_NAME:latest 1111111111.dkr.ecr.us-east-1.amazonaws.com/$STACK_APP_NAME:production_$HASH
echo "Pushing up production image using has production_$HASH"
docker push 1111111111.dkr.ecr.us-east-1.amazonaws.com/$STACK_APP_NAME:production_$HASH
fi</code></pre><p>So Staging will build and push right to the AWS ECR which means Fargate by default will get the Latest tagged image since the TaskDefinition says so. So staging is done. Next task will run this one.</p></p><h2 id="production">Production</h2><p>This we want to happen by choice not by Travis. So you can see the step in Travis<code>STACK_ENV_FILE_PRODUCTION</code> that looks for an environment variable and if true it will push the same working image but with it&rsquo;s own secrets to ECR but with the tag<code>production_GIT_HASH</code></p><p>Then we ready we have a UI to push it BUT really it is just CloudFormation that updates the TaskDefinition using the build in Params to make it reference this HASH. This can be done pretty easily with Python, PHP etc and the AWS SDK that allows you to update CloudFormation and the Parameter that then fills in the TaskDefinition Field and updates it, from there the next time the Production Fargate runs it runs that latest version.</p><p>Example Task Definition:</p><pre><code> "TaskDefinition": {
"Type": "AWS::ECS::TaskDefinition",
"Properties": {
"ExecutionRoleArn": "",
"Memory": 250,
"NetworkMode": "bridge",
"TaskRoleArn": "arn:aws:iam::364215618558:role/foo",
"ContainerDefinitions": [{
"Name": {
"Fn::Sub": "${AppName}-${AppEnv}"
},
"Image": {
"Fn::Sub": "${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${AppName}:${ProductionTag}"
},
"PortMappings": [{
"ContainerPort": 80,
"HostPort": {
"Ref": "AppPort"
}
},
{
"ContainerPort": 443,
"HostPort": {
"Ref": "AppPortSSL"
}
}
],
"Memory": 250,
"MountPoints": [{
"SourceVolume": "shared",
"ContainerPath": "/opt/shared"
}]
}],
"Volumes": [{
"Name": "shared",
"Host": {
"SourcePath": "/opt/shared"
}
}]
},
"DependsOn": [
"ECR"
]
},<p/></pre><p>So when I update this I can just update the<code>ProductionTag</code> and this will take effect.</p></p></description><tags>aws, fargate</tags></item><item><title>Dusk Screenshots to S3 of Failing tests</title><link>https://alfrednutile.info/posts/248/</link><pubDate>Tue, 26 Jun 2018 00:00:00 +0000</pubDate><guid>https://alfrednutile.info/posts/248/</guid><description><p>I can be really annoying to troubleshoot a failed test in Dusk when using CI systems. What I ended up doing was setting up my project to send these files to S3 on fail.</p><p>Here are the steps</p><h2 id="setup-you-app">Setup You App</h2><p>This is just S3 storage so make sure you have a bucket and a folder in the bucket to write to. Basically for AWS you make an IAM with a key and secret and let it read/write to this folder. Might look like this policy</p><pre><code>{
"Version": "2012-10-17",
"Statement": [
{
"Action": [
"s3:ListBucket"
],
"Resource": [
"arn:aws:s3:::your-bucket"
],
"Effect": "Allow"
},
{
"Action": [
"*"
],
"Resource": [
"arn:aws:s3:::your-bucket/*"
],
"Effect": "Allow"
}
]
}</code></pre><p>or limited it just to the folder<code>screenshots</code></p><pre><code>{
"Version": "2012-10-17",
"Statement": [
{
"Action": [
"s3:ListAllMyBuckets",
"s3:HeadBucket"
],
"Resource": "*",
"Effect": "Allow",
"Sid": "VisualEditor1"
},
{
"Action": [
"s3:ListBucket",
"s3:GetBucketLocation"
],
"Resource": [
"arn:aws:s3:::your-bucket",
"arn:aws:s3:::your-bucket/*"
],
"Effect": "Allow",
"Sid": "VisualEditor0"
},
{
"Action": [
"s3:GetObject"
],
"Resource": [
"arn:aws:s3:::your-bucket/screenshots",
"arn:aws:s3:::your-bucket/screenshots/*"
],
"Effect": "Allow",
"Sid": "VisualEditor2"
}
]
}</code></pre><p>Now in your<code>tests/DuskTestCase.php</code> file add this:</p><pre><code> public function tearDown()
{
if (env("TRAVIS")) {
$files = \File::files(base_path("tests/Browser/screenshots"));<pre><code> if ($files) {
foreach ($files as $file) {
Storage::disk('travis_fails')
-&amp;gt;put($file-&amp;gt;getFileName(), $file-&amp;gt;getContents());
}
}
}
parent::tearDown();
}</code></pre><p/></pre><p>This will run on Travis based tests saving files to the bucket.</p></p><p>I added to my<code>config/filesystems.php</code> file this:</p><pre><code> 'disks' => [
'travis_fails' => [
'driver' => 's3',
'key' => env('AWS_ACCESS_KEY_ID'),
'secret' => env('AWS_SECRET_ACCESS_KEY'),
'region' => "eu-west-1",
'root' => "screenshots",
'bucket' => 'foo-bucket',
'url' => env('AWS_URL'),
],
///</code></pre><p>Now for TravisCI</p><h2 id="setup-travisci">Setup TravisCI</h2><p>Then in Travis under Settings -> Environment Variables we make sure to add our key and secret with the proper key format:</p><pre><code>AWS_SECRET_ACCESS_KEY=foo
AWS_ACCESS_KEY_ID=bar</code></pre><p><img src="https://dl.dropboxusercontent.com/s/nez8f4hz4anmghl/Screenshot%202018-06-26%2009.36.26.png?dl=0" alt=""/><h2 id="now-you-are-ready-to-fail">Now You are Ready to Fail!</h2><p>Ok so on your next fail you will see in your s3 bucket some of those super handy screenshots.</p></description><tags>dusk, laravel, testing, aws</tags></item><item><title>Lambda and Github Webhooks</title><link>https://alfrednutile.info/posts/245/</link><pubDate>Tue, 08 May 2018 00:00:00 +0000</pubDate><guid>https://alfrednutile.info/posts/245/</guid><description><p>Related to<a href="https://developer.github.com/webhooks/">https://developer.github.com/webhooks/</a> and pushing data to Lambda AWS.</p><p>I can check the token by just having this check in my handler.</p><pre><code>const crypto = require('crypto');<p>exports.handler = (event, context, callback) => {
let headers = event.headers;</p><p>let body = event.body;</p><p>if (typeof body === &lsquo;object&rsquo;) {
body = JSON.stringify(event.body);
}</p><p>let github_event = headers[&lsquo;X-GitHub-Event&rsquo;];</p><p>if (
github_event === undefined ||
typeof github_event !== &lsquo;string&rsquo; ||
github_event.length &lt; 1
) {
callback(null, {
statusCode: 400,
body: &lsquo;Missing X-GitHub-Event&rsquo;
});
return;
}</p><p>let github_signature = headers[&lsquo;X-Hub-Signature&rsquo;];</p><p>if (
github_signature === undefined ||
typeof github_signature !== &lsquo;string&rsquo; ||
!github_signature.match(/sha1=\S+/)
) {
callback(null, {
statusCode: 400,
body: &lsquo;Missing X-Hub-Signature&rsquo;
});
}</p><p>let secret = process.env.SECRET;</p><p>if (secret === undefined || secret.length &lt; 1) {
callback(null, {
statusCode: 500,
body: &lsquo;We are missing the secret in our server sorry&rsquo;
});
return;
}</p><p>let signature = headers[&lsquo;X-Hub-Signature&rsquo;];</p><p>let computed_signature = crypto
.createHmac(&lsquo;sha1&rsquo;, secret)
.update(body)
.digest(&lsquo;hex&rsquo;);</p><p>if (<code>sha1=${computed_signature}</code> !== signature) {
callback(null, {
statusCode: 403,
body: &lsquo;HMAC Output not Correct&rsquo;
});
return;
}</p><p>//ready to process token
callback(null, {
statusCode: 204,
body: &lsquo;OK&rsquo;
});
};</code></pre><p>With the serverless library from AWS<a href="https://github.com/awslabs/serverless-application-model"><a href="https://github.com/awslabs/serverless-application-model">https://github.com/awslabs/serverless-application-model</a></a> I can build a template to include this in the Resource</p></p><pre><code>Resources:
GithubSecurityWebhook:
Type: AWS::Serverless::Function
DependsOn:
- Stream
Properties:
Runtime: nodejs8.10
Description: >-
This will verify that the incoming data is from Github and hashed with the key
we are using for this environment
Environment:
Variables:
SECRET: !Sub ${Secret}
APP_ENV: !Sub ${AppEnv}
APP_REGION: "eu-west-1"</code></pre><p>then during the update command I can pass</p><pre><code>SECRET=FOO</code></pre><p>or put a .env file on the system and use the<code>dotenv</code> library</p></description><tags>serverless, lambda, aws</tags></item><item><title>WIP AWS Batch and Workers with Laravel</title><link>https://alfrednutile.info/posts/220/</link><pubDate>Wed, 06 Sep 2017 00:00:00 +0000</pubDate><guid>https://alfrednutile.info/posts/220/</guid><description><h1 id="sending-docker-commands-to-get-a-task-done">Sending Docker Commands to Get A Task Done</h1><p>This article will show how to get started with AWS Batch and Docker to spin up a &ldquo;worker&rdquo;</p><p>By the time you are done reading it you will:</p><ol><li>Have a Docker image to run your command in.</li><li>Deploy the Docker image to AWS ECR</li><li>And Run a Task on the AWS Batch system, or a 100 tasks, does not matter.</li><li>Finally you will have the scripts needed to fully build AWS Batch with CloudFormation, e.g. super simple</li></ol><h2 id="building-the-laravel-worker">Building the Laravel Worker</h2><p>This can be any language you want, any framework or none! I will focus on Laravel.</p><p>Either way we need a git repo, so later we can push this and do a Docker build on the CI via the deploy scripts.</p><p>Note the 2 Commands</p><ul><li>One to manage all the Tasks and then stitch the results together</li><li>One to just covert the image to PDF
&ndash; depend on?</li></ul><p>Install SDK</p><h3 id="the-command">The Command</h3><p>Before you get going install<a href="https://aws.amazon.com/cli/">AWS CLI</a> on your machine.</p><p>Make sure you have your credentials and profiles setup per the docs<a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html">here</a></p><p>Then the rest will work off those credentials as we make new ones per application and you just place them into your profile via<code>~/.aws/credentials</code> and<code>~/.aws/config</code></p><h3 id="installing-incomings">Installing Incomings</h3><h2 id="building-the-docker-container-to-run-this-work">Building the Docker container to run this work</h2><pre><code>docker-compose build //this will be needed locally
docker-compose run --rm -w /app app php artisan
docker-compose run app php artisan security:scan --git-repo=https://github.com/alnutile/security-scanner-show-error-poc
docker-compose run -w /app app php artisan batch:convert_image_to_pdf --image-url=https://dl.dropboxusercontent.com/s/d2sx0wjheb7dk0p/example_batch.jpg --destination=batch-example/foo</code></pre><h2 id="setting-up-aws-to-run-the-work-on">Setting up AWS to run the work on.</h2><h3 id="compute-environment">Compute Environment</h3><p>For starters there is the Compute Environment (CE). This is what manages the scaling of the EC2 instances.</p><p>The script is HERE. But this is only needed once. I will Explain in a moment.</p><h3 id="job-definition">Job Definition</h3><p>Then there is the JobDefinitions. This is where we can define the Job we want to run and the Compute Environment to run it on. Let the CE figure out the scaling, and hence we can just build one and share it with every one.
So if you have a CE already build try putting your JobDefinition on that before making another one for your Worker.</p><p>The script is HERE.</p><p>Open the scripts, they are just JSON. Good docs are<strong>HERE</strong> on how to make, or read these scripts.</p><p>Once those are all setup take the AWS KEY and Secret it gave you and use it to setup you local AWS CLI.</p><p>This is key for a sane workflow. Here is a link [<strong>HERE</strong>] get your profile right.</p><p>Once you have this setup all the rest will fall into place.</p><h2 id="deploying-to-ci">Deploying to CI</h2><p>Before we push let&rsquo;s make sure that<code>.gitignore</code> has:</p><pre><code>docker/app/packaged/*
!docker/app/packaged/.gitkeep</code></pre><p>Plus all the default Laravel ignores including<code>.env</code></p><p>Deploy Script
Now we need to get this work to CI</p><h2 id="going-to-aws-ui-to-run">Going to AWS UI to run</h2><p>This is annoying lets make a command for this&hellip;</p><h3 id="the-testing-command">The Testing Command</h3><p>More import that is how a &ldquo;requesting&rdquo; app will call this.</p><p><strong>DIAGRAM HERE</strong></p><h2 id="putting-it-all-together">Putting it all together</h2><p>Show running 10 jobs via the command</p><h2 id="now-taking-it-a-bit-further">Now Taking it a bit further</h2><p>The command did a lot</p><ul><li>Got Image</li><li>Made into PDF</li><li>Stitched them all together</li></ul><p>But now let&rsquo;s take this a bit further. I am going to make 2 JobDefinitions.</p><p>One will kick off an orchestrate the work by creating a job per image, tracking the progress of those jobs, and
When all those jobs are done knit them together!</p><p>Here is how it will look.</p><h2 id="summary-of-install">Summary of Install</h2><p>Get Docker file in place docker/app/DockerFile
Get Packaged file in place
Setup Core CloudFormation</p><ul><li>ECR</li><li>JobDefinition</li><li>User Key/Secret
Get docker-compose inlace
Get Travis.yml in place and docker/deploy.bash
See diagram on how this all comes together</li></ul></description><tags>laravel, note2self, aws, wip</tags></item><item><title>Serverless and Custom Tags for Resources</title><link>https://alfrednutile.info/posts/217/</link><pubDate>Tue, 22 Aug 2017 00:00:00 +0000</pubDate><guid>https://alfrednutile.info/posts/217/</guid><description><p>Because much of serveless.com is CloudFormation based, you can easily update existing resources. They talk about this feature<a href="https://serverless.com/framework/docs/providers/aws/guide/resources/#override-aws-cloudformation-resource">here</a></p><p>For me this was required for adding tags</p><pre><code># you can add CloudFormation resource templates here
resources:
Resources:
ServerlessDeploymentBucket:
Properties:
Tags:
- { Key: "project", Value: "${self:provider.project}" }
- { Key: "environment", Value: "${opt:stage, self:provider.stage}" }
- { Key: "parent_project", Value: "${self:custom.parent}" }
- { Key: "key_contact", Value: "${self:custom.contact}" }
- { Key: "billing_ref", Value: "${self:custom.billing_ref}" }</code></pre><p>now that bucket, that Severless makes by default, will have tags.</p><p>Also I need to tag Lambda functions too for billing:</p><pre><code>functions:
check_queue:
handler: handler.check_queue
tags:
project: ${self:provider.project}
environment: ${opt:stage, self:provider.stage}
parent_project: ${self:custom.parent}
key_contact: ${self:custom.contact}
billing_ref: ${self:custom.billing_ref}
events:</code></pre><p>Some of this info I centralize in this area here</p><pre><code>custom:
secrets: ${file(secrets.${opt:stage, self:provider.stage}.yml)}
contact: AlfredNutile
billing_ref: foo_app
parent: foo_app</code></pre><p>Some of this I can drive using this plugin<code>serverless-secrets-plugin</code><a href="https://www.npmjs.com/package/serverless-secrets-plugin">here</a></p><p>So I then pull in the secrets file and use it as needed.</p><pre><code>provider:
name: aws
stage: dev
runtime: nodejs6.10
environment:
APP_ENV: ${self:custom.secrets.APP_ENV}
REGION: ${self:custom.secrets.REGION}
ACCOUNT_ID: 555555555<p/></pre></p></description><tags>serverless, aws</tags></item><item><title>Serverless, AWS API Gateway and Authentication</title><link>https://alfrednutile.info/posts/214/</link><pubDate>Tue, 18 Jul 2017 00:00:00 +0000</pubDate><guid>https://alfrednutile.info/posts/214/</guid><description><p>Just a note2self really.</p><p>Setting up Auth was super easy</p><p>As seen below I had to add to my serverless.yml<code>authorizer</code></p><pre><code> addQuote:
handler: quote/handler.add
events:
- http:
path: quote
method: post
cors: true
authorizer: aws_iam</code></pre><p><strong>authorizer: aws_iam</strong></p><p>From here I then needed, in this case Postman, to pass an AWS KEY and SECRET made for this app.</p><p><img src="https://dl.dropboxusercontent.com/s/54mz7mxo0x7ei25/auth_aws.jpg?dl=0" alt=""/><p>When making the user I attached this Policy to the user</p><pre><code>{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": [
"execute-api:Invoke"
],
"Resource": "arn:aws:execute-api:us-east-1:AWS_ID:*/dev/POST/quote"
}
]
}</code></pre><p>I an easily make this in the<code>serverless.yml</code> file by adding the following:</p><pre><code>
resources:
Resources:
s3Data:
Type: AWS::S3::Bucket
Properties:
BucketName: "${self:custom.bucket}"
VersioningConfiguration:
Status: Enabled
quotePolicy:
Type: AWS::IAM::Policy
Properties:
PolicyName: "serverless-quotes-policy-${opt:stage, self:provider.stage}"
PolicyDocument:
Version: "2012-10-17"
Statement:
-
Effect: "Allow"
Action:
- "execute-api:Invoke"
Resource: "arn:aws:execute-api:#{AWS::Region}:#{AWS::AccountId}:*/${opt:stage, self:provider.stage}/POST/quote"
Users:
- "serverless-quotes-${opt:stage, self:provider.stage}"
DependsOn:
- authUser
userKey:
Type: AWS::IAM::AccessKey
Properties:
UserName: "serverless-quotes-${opt:stage, self:provider.stage}"
DependsOn:
- authUser
authUser:
Type: AWS::IAM::User
Properties:
UserName: "serverless-quotes-${opt:stage, self:provider.stage}"<p>Outputs:
UserSecret:
Description: The user secret
Value:
"Fn::GetAtt": [ userKey, SecretAccessKey ]
UserKey:
Description: The user key
Value:
"Ref": userKey</code></pre><p>This is after adding<code>serverless-pseudo-parameters</code> plugin, see link below.</p></p><p>Now when I run<code>sls deploy --stage dev</code> I get the User, IAM, Key and Secret (via the webui output), then I need to access this url.</p><p>That was it. With serverless.com I could have limited stage, production builds to the unique user per stack as well.</p><h2 id="links">Links</h2><p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/iam-authentication-api-gateway/">https://aws.amazon.com/premiumsupport/knowledge-center/iam-authentication-api-gateway/</a></p><p><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudo-parameter-reference.html">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudo-parameter-reference.html</a></p><p><a href="https://serverless.com/framework/docs/providers/aws/guide/serverless.yml/#serverlessyml-reference">https://serverless.com/framework/docs/providers/aws/guide/serverless.yml/#serverlessyml-reference</a></p></description><tags>serverless.com, aws, serverless</tags></item><item><title>AWS, S3 Storage and limited Visibility</title><link>https://alfrednutile.info/posts/180/</link><pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate><guid>https://alfrednutile.info/posts/180/</guid><description><p>Storing files on S3 is great. And many times those files are private so I only want to give temporary access to them. Using Laravel Storage I can interact with the file-system quite easily and seamlessly especially between Cloud and Local.</p><p>In this case I want to return a file only for 10 minutes</p><pre><code> public function getSignedUrl($filename_and_path, $expires_minutes = '10')
{
$client = Storage::disk('s3')->getDriver()->getAdapter()->getClient();
$bucket = env('BUCKET');<pre><code> $command = $client-&amp;gt;getCommand('GetObject', [
'Bucket' =&amp;gt; $bucket,
'Key' =&amp;gt; $filename_and_path
]);
$request = $client-&amp;gt;createPresignedRequest($command, Carbon::now()-&amp;gt;addMinutes($expires_minutes));
return (string) $request-&amp;gt;getUri();
}</code></pre><p/></pre><p>That is it!</p></p><p>Thanks to this<a href="https://github.com/minio/minio/issues/1285">post</a> for the help.</p></description><tags>php, aws</tags></item></channel></rss>